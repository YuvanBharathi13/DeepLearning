{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPygEaVcPU3HzRyGAnMTiXh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuvanBharathi13/DeepLearning/blob/main/01_neural_network_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Regression with Neural Networks in TensorFlow"
      ],
      "metadata": {
        "id": "Bnp2IZ2mUtDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "tMv-L2nGVc9N"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating new data to view it"
      ],
      "metadata": {
        "id": "kpCFEXoOWMNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "X = np.array([-7.0,-4.0,-1.0,2.0,5.0,8.0,11.0,14.0])\n",
        "y = np.array([3.0,6.0,9.0,12.0,15.0,18.0,21.0,24.0])"
      ],
      "metadata": {
        "id": "-hnl1ilsVfAx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X,y)\n",
        "plt.xlabel(\"X-axis\")\n",
        "plt.ylabel(\"Y-axis\")\n",
        "plt.title(\"Sample data\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "LS4D3o0wWrdU",
        "outputId": "466464eb-02be-4a53-ee0e-8586e4b053c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y == X + 10 # from this we can tell that the equation of the line would be y = X +10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0op3SR1Wtnm",
        "outputId": "6086db18-f4f5-48e2-d7fd-86e672accd7d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True,  True,  True,  True,  True,  True])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input and Output shapes"
      ],
      "metadata": {
        "id": "ZyY5v5U9XpZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# our input and output shapes are going to be 1\n",
        "X[0].shape, y[0].shape # its showing nothing cause when we are accesing 1 value from an array it returns a scalar."
      ],
      "metadata": {
        "id": "YciGfm4uXErh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74cfb518-6b58-45c5-f5ad-3e51c735e65d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((), ())"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0].ndim # this shows that it is a scalar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q2ci6yzZLVb",
        "outputId": "a2f30091-f634-4fbf-e075-51f566527fab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# turning the numpy arrays into tensors\n",
        "X = tf.constant(X)\n",
        "y = tf.constant(y)\n",
        "X,y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQumdMGSZab0",
        "outputId": "a1689e78-0920-499f-c12a-6f542516cec0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(8,), dtype=float64, numpy=array([-7., -4., -1.,  2.,  5.,  8., 11., 14.])>,\n",
              " <tf.Tensor: shape=(8,), dtype=float64, numpy=array([ 3.,  6.,  9., 12., 15., 18., 21., 24.])>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps in creating a model in tensorflow\n",
        "1. **Creating the model** - defining the input, output layers and alos the hidden layers.\n",
        "2. **Compiling the model** - defining the loss function that will correct out model, choosing the optimzer that tells our model how to improve the patterns it is learning and evaluation metrics thats tells us the performance of our model.\n",
        "3. **Fitting the model** - Letting the model find patterns in X and y which are our features and labels."
      ],
      "metadata": {
        "id": "7IJ6ISSucs7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.ndim, tf.expand_dims(X, axis=-1).ndim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33oW4fQZgxGf",
        "outputId": "6e472eef-73ed-4149-decb-8248af81cf52"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_new = tf.expand_dims(X, axis=-1)\n",
        "x_new[1,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baZh50bwXEWz",
        "outputId": "a2783f86-7cbd-4c1a-eb54-b3cef9bed7f1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float64, numpy=-4.0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. creating the model using the sequential API\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1,input_shape=(1,))\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model.compile(loss = tf.keras.losses.mae,\n",
        "              optimizer = tf.keras.optimizers.SGD(),\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "# 3.fit the model\n",
        "model.fit(X,y,epochs=5) # since i have provided the input size i do not need to change the tensor to 2 dimensions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9e6K7I47eDiR",
        "outputId": "586c98aa-8edd-4a63-c2fc-4fc000ca0438"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419ms/step - loss: 15.6170 - mae: 15.6170\n",
            "Epoch 2/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 15.3358 - mae: 15.3358\n",
            "Epoch 3/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 15.0545 - mae: 15.0545\n",
            "Epoch 4/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 14.8376 - mae: 14.8376\n",
            "Epoch 5/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 14.7051 - mae: 14.7051\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7def0cb67e50>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting using a sample value\n",
        "chumma = tf.constant([[17.0]]) # make sure the input is always a tensor\n",
        "model.predict(chumma) # this is when the epochs is 5, has a MAE of 7.4738"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuxjRdI8gLIZ",
        "outputId": "4e6c6e61-14aa-48f9-b3b3-b5f4ac5efd1e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7def0cd1d080> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[29.220205]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improving the Model\n",
        "Places to improve the model -\n",
        "1. **Creating a model**- in this step in order to improve our model we may increase the **number of hidden layers**, we may **increase the number of neurons per layer** or change our **activation function** (in this case its linear by default).\n",
        "2.**Compiling the model** - here we might the change the optmization function or the **learning rate**.\n",
        "3. **Fitting a model** - here we might increase the number of **epochs** or give the model more **data**."
      ],
      "metadata": {
        "id": "lXTwydija4JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Improving the model only by increasing the epoch number"
      ],
      "metadata": {
        "id": "Sjy1ip-DlanR"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. creating the model using the sequential API\n",
        "model_epoch = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_epoch.compile(loss = tf.keras.losses.mae,\n",
        "              optimizer = tf.keras.optimizers.SGD(),\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "# 3.fit the model\n",
        "model_epoch.fit(tf.expand_dims(X, axis=-1),y,epochs=100) # Since i have not proved the input shape,\n",
        "# i will have to convert my 1 dimensional tensor to 2 dimensions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WzkD1lypbNJ1",
        "outputId": "47993dee-fe99-4c28-c850-daa57b470eff"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 638ms/step - loss: 18.4773 - mae: 18.4773\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 18.1961 - mae: 18.1961\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 17.9148 - mae: 17.9148\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 17.6336 - mae: 17.6336\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 17.3523 - mae: 17.3523\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 17.0711 - mae: 17.0711\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 16.7898 - mae: 16.7898\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 16.5086 - mae: 16.5086\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 16.2273 - mae: 16.2273\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 15.9461 - mae: 15.9461\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 15.6648 - mae: 15.6648\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 15.3836 - mae: 15.3836\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 15.1023 - mae: 15.1023\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 14.8320 - mae: 14.8320\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 14.6995 - mae: 14.6995\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 14.5670 - mae: 14.5670\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 14.4345 - mae: 14.4345\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 14.3020 - mae: 14.3020\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 14.1695 - mae: 14.1695\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 14.0370 - mae: 14.0370\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 13.9045 - mae: 13.9045\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 13.7720 - mae: 13.7720\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 13.6395 - mae: 13.6395\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 13.5070 - mae: 13.5070\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 13.3745 - mae: 13.3745\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 13.2420 - mae: 13.2420\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 13.1095 - mae: 13.1095\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 12.9770 - mae: 12.9770\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 12.8445 - mae: 12.8445\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 12.7120 - mae: 12.7120\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 12.5795 - mae: 12.5795\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 12.4470 - mae: 12.4470\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 12.3145 - mae: 12.3145\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 12.1820 - mae: 12.1820\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 12.0495 - mae: 12.0495\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 11.9170 - mae: 11.9170\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 11.7845 - mae: 11.7845\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 11.6520 - mae: 11.6520\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 11.5195 - mae: 11.5195\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 11.3870 - mae: 11.3870\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 11.2545 - mae: 11.2545\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 11.1220 - mae: 11.1220\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 10.9895 - mae: 10.9895\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 10.8570 - mae: 10.8570\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 10.7245 - mae: 10.7245\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 10.5920 - mae: 10.5920\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 10.4595 - mae: 10.4595\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 10.3270 - mae: 10.3270\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 10.1945 - mae: 10.1945\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 10.0620 - mae: 10.0620\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 9.9295 - mae: 9.9295\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 9.7970 - mae: 9.7970\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 9.6645 - mae: 9.6645\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 9.5320 - mae: 9.5320\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 9.3995 - mae: 9.3995\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 9.2670 - mae: 9.2670\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 9.1345 - mae: 9.1345\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 9.0020 - mae: 9.0020\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 8.8695 - mae: 8.8695\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 8.7370 - mae: 8.7370\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 8.6045 - mae: 8.6045\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 8.4720 - mae: 8.4720\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 8.3395 - mae: 8.3395\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 8.2070 - mae: 8.2070\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 8.0745 - mae: 8.0745\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 7.9420 - mae: 7.9420\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 7.8095 - mae: 7.8095\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 7.6770 - mae: 7.6770\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 7.5445 - mae: 7.5445\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 7.4120 - mae: 7.4120\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 7.2795 - mae: 7.2795\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 7.1470 - mae: 7.1470\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 7.0145 - mae: 7.0145\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 6.9769 - mae: 6.9769\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 6.9713 - mae: 6.9713\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 6.9656 - mae: 6.9656\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 6.9600 - mae: 6.9600\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 6.9544 - mae: 6.9544\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 6.9488 - mae: 6.9488\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 6.9431 - mae: 6.9431\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 6.9375 - mae: 6.9375\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 6.9319 - mae: 6.9319\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 6.9263 - mae: 6.9263\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 6.9206 - mae: 6.9206\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 6.9150 - mae: 6.9150\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 6.9094 - mae: 6.9094\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 6.9038 - mae: 6.9038\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 6.8981 - mae: 6.8981\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 6.8925 - mae: 6.8925\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 6.8869 - mae: 6.8869\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 6.8813 - mae: 6.8813\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 6.8756 - mae: 6.8756\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 6.8700 - mae: 6.8700\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 6.8644 - mae: 6.8644\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 6.8588 - mae: 6.8588\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 6.8531 - mae: 6.8531\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 6.8475 - mae: 6.8475\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 6.8419 - mae: 6.8419\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 6.8363 - mae: 6.8363\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 6.8306 - mae: 6.8306\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7def0ca8b690>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chumma = tf.constant([[17.0]])\n",
        "model_epoch.predict(chumma) # this is when the epochs was 100, has a MAE of 6.8306"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ2fO8FBWrAu",
        "outputId": "b4468b06-d2e2-44bc-d0eb-fe42258c74be"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[29.6568]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the optimizer to Adam"
      ],
      "metadata": {
        "id": "_LefhDuimxk0"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. creating the model using the sequential API\n",
        "model_adam = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_adam.compile(loss = tf.keras.losses.mae,\n",
        "              optimizer = tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "# 3.fit the model\n",
        "model_adam.fit(tf.expand_dims(X, axis=-1),y,epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "l4o8FnNhaCwp",
        "outputId": "c1bb9b5f-695b-48cf-cf84-d22d945840d0"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step - loss: 18.9337 - mae: 18.9337\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 18.9277 - mae: 18.9277\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 18.9217 - mae: 18.9217\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 18.9157 - mae: 18.9157\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 18.9097 - mae: 18.9097\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 18.9037 - mae: 18.9037\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 18.8977 - mae: 18.8977\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 18.8917 - mae: 18.8917\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 18.8857 - mae: 18.8857\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 18.8797 - mae: 18.8797\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 18.8737 - mae: 18.8737\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 18.8677 - mae: 18.8677\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 18.8617 - mae: 18.8617\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 18.8557 - mae: 18.8557\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 18.8497 - mae: 18.8497\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 18.8437 - mae: 18.8437\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 18.8377 - mae: 18.8377\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 18.8317 - mae: 18.8317\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 18.8257 - mae: 18.8257\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 18.8197 - mae: 18.8197\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 18.8137 - mae: 18.8137\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 18.8077 - mae: 18.8077\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 18.8017 - mae: 18.8017\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 18.7957 - mae: 18.7957\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 18.7897 - mae: 18.7897\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 18.7837 - mae: 18.7837\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 18.7777 - mae: 18.7777\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 18.7717 - mae: 18.7717\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 18.7657 - mae: 18.7657\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 18.7597 - mae: 18.7597\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 18.7537 - mae: 18.7537\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 18.7477 - mae: 18.7477\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 18.7417 - mae: 18.7417\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 18.7357 - mae: 18.7357\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 18.7297 - mae: 18.7297\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 18.7237 - mae: 18.7237\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 18.7177 - mae: 18.7177\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 18.7117 - mae: 18.7117\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 18.7057 - mae: 18.7057\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 18.6997 - mae: 18.6997\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 18.6937 - mae: 18.6937\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 18.6877 - mae: 18.6877\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 18.6817 - mae: 18.6817\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 18.6757 - mae: 18.6757\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 18.6697 - mae: 18.6697\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 18.6637 - mae: 18.6637\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 18.6577 - mae: 18.6577\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 18.6517 - mae: 18.6517\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 18.6457 - mae: 18.6457\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 18.6397 - mae: 18.6397\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 18.6337 - mae: 18.6337\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 18.6277 - mae: 18.6277\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 18.6217 - mae: 18.6217\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 18.6157 - mae: 18.6157\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 18.6097 - mae: 18.6097\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 18.6037 - mae: 18.6037\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 18.5977 - mae: 18.5977\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 18.5917 - mae: 18.5917\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 18.5857 - mae: 18.5857\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 18.5797 - mae: 18.5797\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 18.5737 - mae: 18.5737\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 18.5677 - mae: 18.5677\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 18.5617 - mae: 18.5617\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 18.5557 - mae: 18.5557\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 18.5497 - mae: 18.5497\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 18.5437 - mae: 18.5437\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 18.5377 - mae: 18.5377\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 18.5317 - mae: 18.5317\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 18.5257 - mae: 18.5257\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 18.5197 - mae: 18.5197\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 18.5137 - mae: 18.5137\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 18.5077 - mae: 18.5077\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 18.5017 - mae: 18.5017\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 18.4957 - mae: 18.4957\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 18.4897 - mae: 18.4897\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 18.4837 - mae: 18.4837\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 18.4777 - mae: 18.4777\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 18.4717 - mae: 18.4717\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 18.4657 - mae: 18.4657\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 18.4597 - mae: 18.4597\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 18.4537 - mae: 18.4537\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 18.4477 - mae: 18.4477\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 18.4417 - mae: 18.4417\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 18.4357 - mae: 18.4357\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 18.4297 - mae: 18.4297\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 18.4237 - mae: 18.4237\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 18.4177 - mae: 18.4177\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 18.4117 - mae: 18.4117\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 18.4057 - mae: 18.4057\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 18.3997 - mae: 18.3997\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 18.3937 - mae: 18.3937\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 18.3877 - mae: 18.3877\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 18.3817 - mae: 18.3817\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 18.3757 - mae: 18.3757\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 18.3697 - mae: 18.3697\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 18.3637 - mae: 18.3637\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 18.3577 - mae: 18.3577\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 18.3517 - mae: 18.3517\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 18.3457 - mae: 18.3457\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 18.3397 - mae: 18.3397\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7def0c94ef90>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_adam.predict(tf.constant([17.0])) # we can see that switching to adam optimzer has made it worse than SGD in this case.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0d-ijO8nImM",
        "outputId": "2dfe8311-4098-4365-a8d4-67088501d00f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-18.223307]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Improving with another hidden layer"
      ],
      "metadata": {
        "id": "S_rzomgLpfVm"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_layer = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100,activation=\"relu\",input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model_layer.compile(loss = tf.keras.losses.mae,\n",
        "                     optimizer=tf.keras.optimizers.SGD(),\n",
        "                     metrics=[\"mae\"])\n",
        "\n",
        "model_layer.fit(X,y,epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HXEK12ITogpe",
        "outputId": "baa6dae9-d116-4fce-d77c-6728024ce1ed"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446ms/step - loss: 12.2774 - mae: 12.2774\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 11.7708 - mae: 11.7708\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 11.2495 - mae: 11.2495\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 10.7085 - mae: 10.7085\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 10.1428 - mae: 10.1428\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 9.5483 - mae: 9.5483\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 8.9232 - mae: 8.9232\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 8.2615 - mae: 8.2615\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 7.5541 - mae: 7.5541\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 6.8037 - mae: 6.8037\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 6.0056 - mae: 6.0056\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 5.1406 - mae: 5.1406\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 4.2461 - mae: 4.2461\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 4.1005 - mae: 4.1005\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 4.0080 - mae: 4.0080\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.9121 - mae: 3.9121\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 3.9736 - mae: 3.9736\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 3.9077 - mae: 3.9077\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 3.9579 - mae: 3.9579\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 3.9139 - mae: 3.9139\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 3.9326 - mae: 3.9326\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 3.9261 - mae: 3.9261\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 3.9157 - mae: 3.9157\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 3.9357 - mae: 3.9357\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 3.8901 - mae: 3.8901\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 3.9421 - mae: 3.9421\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 3.8767 - mae: 3.8767\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 3.9354 - mae: 3.9354\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 3.8831 - mae: 3.8831\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 3.9154 - mae: 3.9154\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 3.8987 - mae: 3.8987\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 3.8927 - mae: 3.8927\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 3.9052 - mae: 3.9052\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 3.8668 - mae: 3.8668\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 3.9119 - mae: 3.9119\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 3.8469 - mae: 3.8469\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 3.9116 - mae: 3.9116\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 3.8579 - mae: 3.8579\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 3.8943 - mae: 3.8943\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 3.8692 - mae: 3.8692\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.8684 - mae: 3.8684\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 3.8759 - mae: 3.8759\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 3.8422 - mae: 3.8422\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 3.8828 - mae: 3.8828\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.8182 - mae: 3.8182\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 3.8904 - mae: 3.8904\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 3.8340 - mae: 3.8340\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 3.8690 - mae: 3.8690\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 3.8408 - mae: 3.8408\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 3.8428 - mae: 3.8428\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 3.8478 - mae: 3.8478\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 3.8164 - mae: 3.8164\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 3.8549 - mae: 3.8549\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 3.7930 - mae: 3.7930\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.8688 - mae: 3.8688\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 3.8066 - mae: 3.8066\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 3.8424 - mae: 3.8424\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 3.8136 - mae: 3.8136\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 3.8159 - mae: 3.8159\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 3.8208 - mae: 3.8208\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 3.7892 - mae: 3.7892\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 3.8289 - mae: 3.8289\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 3.7731 - mae: 3.7731\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 3.8412 - mae: 3.8412\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 3.7802 - mae: 3.7802\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 3.8146 - mae: 3.8146\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 3.7874 - mae: 3.7874\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 3.7877 - mae: 3.7877\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 3.7948 - mae: 3.7948\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 3.7608 - mae: 3.7608\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 3.8105 - mae: 3.8105\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 3.7474 - mae: 3.7474\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.8125 - mae: 3.8125\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.7547 - mae: 3.7547\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 3.7856 - mae: 3.7856\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 3.7622 - mae: 3.7622\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 3.7585 - mae: 3.7585\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 3.7697 - mae: 3.7697\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 3.7384 - mae: 3.7384\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 3.7864 - mae: 3.7864\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 3.7228 - mae: 3.7228\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.7826 - mae: 3.7826\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 3.7303 - mae: 3.7303\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 3.7553 - mae: 3.7553\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 3.7379 - mae: 3.7379\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 3.7279 - mae: 3.7279\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.7510 - mae: 3.7510\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 3.7087 - mae: 3.7087\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 3.7625 - mae: 3.7625\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.6991 - mae: 3.6991\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 3.7513 - mae: 3.7513\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 3.7068 - mae: 3.7068\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.7237 - mae: 3.7237\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 3.7147 - mae: 3.7147\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 3.7000 - mae: 3.7000\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 3.7316 - mae: 3.7316\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 3.6765 - mae: 3.6765\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.7395 - mae: 3.7395\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 3.6763 - mae: 3.6763\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 3.7188 - mae: 3.7188\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7deefbe03910>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_layer.predict(tf.constant([17.0])) # we can see how much our MAE has improved from when we had only 1 layer.\n",
        "# Even though the MAE is better, the prediction is worse than the previous one."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOkQAbLjqNoR",
        "outputId": "7c104e89-4544-4290-8947-571f65fa7dfa"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[32.305237]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, what might be happening in the above case where the training metric has improved but the prediction has worsened is that by increasing the number of layers or increasing the neurons per layer we are over fitting the data and that may cause the prediction value to be worse than before."
      ],
      "metadata": {
        "id": "xnHRwY2Prkx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note - The learning rate is the most important hyperparameter that we can change to improve our model."
      ],
      "metadata": {
        "id": "we-AC9ZXtNnz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l7DQK88Ctnko"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}